#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è­°äº‹éŒ²ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ–­ç‰‡åŒ–ã—ã¦ChromaDBã«ä¿å­˜ã™ã‚‹ãƒ—ãƒ­ã‚°ãƒ©ãƒ 

ä¾å­˜é–¢ä¿‚:
- chromadb
- sentence-transformers (æ¨å¥¨ã€ã‚ˆã‚Šé«˜å“è³ªãªåŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’ç”Ÿæˆ)

ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«:
pip install sentence-transformers
"""

import os
import re
import json
from datetime import datetime
from typing import List, Dict, Any
import chromadb
from chromadb.config import Settings
import chromadb.utils.embedding_functions
import markdown
from pathlib import Path
import numpy as np

# sentence-transformersã‚’ç›´æ¥ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from sentence_transformers import SentenceTransformer
    SENTENCE_TRANSFORMERS_AVAILABLE = True
except ImportError:
    SENTENCE_TRANSFORMERS_AVAILABLE = False
    print("âš ï¸  sentence-transformersãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“")


class MeetingRAGProcessor:
    """è­°äº‹éŒ²ã‚’æ–­ç‰‡åŒ–ã—ã¦ChromaDBã«ä¿å­˜ã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, db_path: str = "./chroma_db"):
        """
        åˆæœŸåŒ–
        
        Args:
            db_path: ChromaDBã®ä¿å­˜ãƒ‘ã‚¹
        """
        self.db_path = db_path
        self.client = chromadb.PersistentClient(
            path=db_path,
            settings=Settings(anonymized_telemetry=False)
        )
        
        # ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å
        self.collection_name = "meeting_minutes"
        
        # åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–
        self.embedding_model = None
        self.embedding_dimension = 384  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæ¬¡å…ƒæ•°

        if not SENTENCE_TRANSFORMERS_AVAILABLE:
            raise RuntimeError("sentence-transformersãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚RAGç™»éŒ²ã‚’ä¸­æ­¢ã—ã¾ã™ã€‚")

        if SENTENCE_TRANSFORMERS_AVAILABLE:
            try:
                print("ğŸ”„ SentenceTransformerãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–ä¸­...")
                self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
                self.embedding_dimension = self.embedding_model.get_sentence_embedding_dimension()
                print(f"âœ… SentenceTransformerãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–å®Œäº†ï¼ˆæ¬¡å…ƒæ•°: {self.embedding_dimension}ï¼‰")
            except Exception as e:
                print(f"âš ï¸  SentenceTransformerã®åˆæœŸåŒ–ã«å¤±æ•—: {e}")
                print("ğŸ”„ ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®åŸ‹ã‚è¾¼ã¿é–¢æ•°ã‚’ä½¿ç”¨ã—ã¾ã™")
        else:
            print("ğŸ”„ ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®åŸ‹ã‚è¾¼ã¿é–¢æ•°ã‚’ä½¿ç”¨ã—ã¾ã™")
        
        # ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã®å–å¾—ã¾ãŸã¯ä½œæˆ
        self._setup_collection()
    
    def _setup_collection(self):
        """ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã®è¨­å®šã‚’è¡Œã†"""
        try:
            self.collection = self.client.get_collection(name=self.collection_name)
            print(f"æ—¢å­˜ã®ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ '{self.collection_name}' ã‚’å–å¾—ã—ã¾ã—ãŸ")
            if not self._verify_collection_embeddings():
                raise RuntimeError("æ—¢å­˜ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã®åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«æ¬¡å…ƒæ•°ãŒä¸ä¸€è‡´ã§ã™ã€‚å®‰å…¨ã®ãŸã‚ç™»éŒ²ã‚’ä¸­æ­¢ã—ã¾ã™ã€‚")
        except chromadb.errors.NotFoundError:
            print(f"ğŸ”„ æ–°ã—ã„ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’ä½œæˆä¸­...")
            self._create_new_collection()
        # ãã‚Œä»¥å¤–ã®ä¾‹å¤–ã¯å†ã‚¹ãƒ­ãƒ¼ï¼ˆæ–°è¦ä½œæˆã—ãªã„ï¼‰
        except Exception as e:
            raise
    
    def _verify_collection_embeddings(self) -> bool:
        """ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã®åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ãŒæ­£å¸¸ã«å‹•ä½œã™ã‚‹ã‹ç¢ºèª"""
        try:
            if self.collection.count() == 0:
                print("âœ… ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ãŒç©ºã§ã™ï¼ˆæ–°è¦ä½œæˆç›´å¾Œï¼‰ã€‚")
                return True
            test_text = "ãƒ†ã‚¹ãƒˆ"
            test_embedding = self.embedding_model.encode(test_text).tolist()
            test_result = self.collection.query(
                query_embeddings=[test_embedding],
                n_results=1,
                include=["embeddings", "documents", "metadatas", "distances"]
            )
            #print("DEBUG: test_result['embeddings'] =", test_result["embeddings"])
            if (test_result and 
                "embeddings" in test_result and 
                test_result["embeddings"] and 
                len(test_result["embeddings"]) > 0):
                first_embedding = test_result["embeddings"][0]
                #print("DEBUG: first_embedding =", first_embedding)
                # NumPyé…åˆ—å¯¾å¿œ
                if hasattr(first_embedding, "shape"):
                    # 2æ¬¡å…ƒé…åˆ— (1, 384) ã®å ´åˆ
                    if len(first_embedding.shape) == 2 and first_embedding.shape[0] == 1:
                        actual_dim = first_embedding.shape[1]
                        print(f"DEBUG: Detected numpy 2D array, shape={first_embedding.shape}, using shape[1]={actual_dim}")
                    else:
                        actual_dim = first_embedding.shape[0]
                        print(f"DEBUG: Detected numpy array, shape={first_embedding.shape}, using shape[0]={actual_dim}")
                elif isinstance(first_embedding, list):
                    # ãƒªã‚¹ãƒˆã®ãƒªã‚¹ãƒˆã®å ´åˆ
                    if len(first_embedding) == 1 and isinstance(first_embedding[0], (list, np.ndarray)):
                        actual_dim = len(first_embedding[0])
                        print(f"DEBUG: Detected list of list, using len(first_embedding[0])={actual_dim}")
                    else:
                        actual_dim = len(first_embedding)
                        print(f"DEBUG: Detected list, using len(first_embedding)={actual_dim}")
                else:
                    actual_dim = len(first_embedding)
                    print(f"DEBUG: Fallback, using len(first_embedding)={actual_dim}")

                if actual_dim == self.embedding_dimension:
                    print(f"âœ… åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã®æ¬¡å…ƒæ•°ãŒä¸€è‡´ã—ã¦ã„ã¾ã™ï¼ˆ{actual_dim}ï¼‰")
                    return True
                else:
                    print(f"âš ï¸  åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã®æ¬¡å…ƒæ•°ãŒä¸ä¸€è‡´: æœŸå¾…å€¤={self.embedding_dimension}, å®Ÿéš›={actual_dim}")
                    return False
            else:
                print("âœ… ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ãŒç©ºã§ã™ï¼ˆæ–°è¦ä½œæˆç›´å¾Œï¼‰ã€‚")
                return True
        except Exception as e:
            print(f"âš ï¸  ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã®æ¤œè¨¼ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def _recreate_collection(self):
        """ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’å†ä½œæˆã™ã‚‹"""
        try:
            print("ğŸ”„ æ—¢å­˜ã®ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’å‰Šé™¤ä¸­...")
            self.client.delete_collection(name=self.collection_name)
            print("âœ… æ—¢å­˜ã®ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’å‰Šé™¤ã—ã¾ã—ãŸ")
            self._create_new_collection()
        except Exception as e:
            print(f"âŒ ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã®å‰Šé™¤ã«å¤±æ•—: {e}")
            raise e
    
    def _create_new_collection(self):
        """æ–°ã—ã„ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’ä½œæˆã™ã‚‹"""
        try:
            print("ğŸ”„ æ–°ã—ã„ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’ä½œæˆä¸­...")
            self.collection = self.client.create_collection(
                name=self.collection_name,
                metadata={
                    "description": "ä¼šè­°è­°äº‹éŒ²ã®æ–­ç‰‡åŒ–ãƒ‡ãƒ¼ã‚¿",
                    "embedding_dimension": self.embedding_dimension
                }
            )
            print(f"âœ… æ–°ã—ã„ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ '{self.collection_name}' ã‚’ä½œæˆã—ã¾ã—ãŸ")
        except Exception as e:
            print(f"âŒ ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã®ä½œæˆã«å¤±æ•—: {e}")
            raise e
    
    def parse_markdown_file(self, file_path: str) -> Dict[str, Any]:
        """
        Markdownãƒ•ã‚¡ã‚¤ãƒ«ã‚’è§£æã—ã¦æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã«å¤‰æ›
        
        Args:
            file_path: Markdownãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
            
        Returns:
            è§£æçµæœã®è¾æ›¸
        """
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰åŸºæœ¬æƒ…å ±ã‚’æŠ½å‡º
        file_name = Path(file_path).stem
        
        # è­°äº‹éŒ²ã®åŸºæœ¬æ§‹é€ ã‚’è§£æ
        parsed_data = {
            "file_name": file_name,
            "raw_content": content,
            "sections": [],
            "metadata": {}
        }
        
        # ãƒ˜ãƒƒãƒ€ãƒ¼æƒ…å ±ã‚’æŠ½å‡º
        header_match = re.search(r'# ä¼šè­°è­°äº‹éŒ²\s*\n\n(.*?)\n\n---', content, re.DOTALL)
        if header_match:
            header_text = header_match.group(1)
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
            metadata = self._extract_metadata(header_text)
            parsed_data["metadata"] = metadata
        
        # ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’åˆ†å‰²
        sections = self._split_into_sections(content)
        parsed_data["sections"] = sections
        
        return parsed_data
    
    def _extract_metadata(self, header_text: str) -> Dict[str, Any]:
        """
        ãƒ˜ãƒƒãƒ€ãƒ¼ã‹ã‚‰ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
        
        Args:
            header_text: ãƒ˜ãƒƒãƒ€ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆ
            
        Returns:
            ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®è¾æ›¸
        """
        metadata = {}
        
        # å…ƒãƒ•ã‚¡ã‚¤ãƒ«
        if match := re.search(r'\*\*å…ƒãƒ•ã‚¡ã‚¤ãƒ«\*\*: (.+)', header_text):
            metadata["source_file"] = match.group(1).strip()
        
        # ä½œæˆæ—¥æ™‚
        if match := re.search(r'\*\*ä½œæˆæ—¥æ™‚\*\*: (.+)', header_text):
            date_str = match.group(1).strip()
            metadata["created_date"] = date_str
        
        # ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«
        if match := re.search(r'\*\*ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«\*\*: (.+)', header_text):
            metadata["model_used"] = match.group(1).strip()
        
        # è­°äº‹éŒ²æ–‡å­—æ•°
        if match := re.search(r'\*\*è­°äº‹éŒ²æ–‡å­—æ•°\*\*: (\d+)æ–‡å­—', header_text):
            metadata["character_count"] = int(match.group(1))
        
        return metadata
    
    def _split_into_sections(self, content: str) -> List[Dict[str, Any]]:
        """
        ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«åˆ†å‰²
        
        Args:
            content: è­°äº‹éŒ²ã®å†…å®¹
            
        Returns:
            ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®ãƒªã‚¹ãƒˆ
        """
        sections = []
        
        # ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å®šç¾©
        section_patterns = [
            (r'# 1\. ä¼šè­°æ¦‚è¦\s*\n(.*?)(?=\n# 2\.|\n---)', "meeting_summary", "ä¼šè­°æ¦‚è¦"),
            (r'# 2\. ä¸»è¦ãªè­°é¡Œã¨è­°è«–å†…å®¹\s*\n(.*?)(?=\n# 3\.|\n---)', "discussion_topics", "ä¸»è¦ãªè­°é¡Œã¨è­°è«–å†…å®¹"),
            (r'# 3\. æ±ºå®šäº‹é …ãƒ»çµè«–\s*\n(.*?)(?=\n# 4\.|\n---)', "decisions", "æ±ºå®šäº‹é …ãƒ»çµè«–"),
            (r'# 4\. ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚¢ã‚¤ãƒ†ãƒ \s*\n(.*?)(?=\n# 5\.|\n---)', "action_items", "ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚¢ã‚¤ãƒ†ãƒ "),
            (r'# 5\. æ¬¡å›æ¤œè¨äº‹é …\s*\n(.*?)(?=\n---|\Z)', "next_agenda", "æ¬¡å›æ¤œè¨äº‹é …")
        ]
        
        for pattern, section_type, section_title in section_patterns:
            match = re.search(pattern, content, re.DOTALL)
            if match:
                section_content = match.group(1).strip()
                
                # ã‚µãƒ–ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«ã•ã‚‰ã«åˆ†å‰²
                subsections = self._split_subsections(section_content, section_type)
                
                for i, subsection in enumerate(subsections):
                    sections.append({
                        "type": section_type,
                        "title": section_title,
                        "subtitle": subsection.get("subtitle", ""),
                        "content": subsection["content"],
                        "content_type": subsection.get("content_type", "text")
                    })
        
        return sections
    
    def _split_subsections(self, content: str, section_type: str) -> List[Dict[str, Any]]:
        """
        ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ã‚µãƒ–ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«åˆ†å‰²
        
        Args:
            content: ã‚»ã‚¯ã‚·ãƒ§ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
            section_type: ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚¿ã‚¤ãƒ—
            
        Returns:
            ã‚µãƒ–ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®ãƒªã‚¹ãƒˆ
        """
        subsections = []
        
        if section_type == "discussion_topics":
            # ä¸»è¦ãªè­°é¡Œã¯2.1, 2.2, 2.3ã§åˆ†å‰²
            topic_pattern = r'## (2\.\d+ [^\n]+)\s*\n(.*?)(?=\n## 2\.\d+|\Z)'
            matches = re.findall(topic_pattern, content, re.DOTALL)
            
            for title, topic_content in matches:
                subsections.append({
                    "subtitle": title.strip(),
                    "content": topic_content.strip(),
                    "content_type": "discussion_topic"
                })
        
        elif section_type == "action_items":
            # ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚¢ã‚¤ãƒ†ãƒ ã¯ãƒ†ãƒ¼ãƒ–ãƒ«å½¢å¼ãªã®ã§ã€è¡Œã”ã¨ã«åˆ†å‰²
            lines = content.split('\n')
            table_started = False
            current_items = []
            
            for line in lines:
                if '|' in line and '---' not in line:
                    if not table_started:
                        table_started = True
                        continue
                    
                    # ãƒ†ãƒ¼ãƒ–ãƒ«è¡Œã‚’è§£æ
                    if line.strip() and not line.startswith('|'):
                        continue
                    
                    cells = [cell.strip() for cell in line.split('|')[1:-1]]
                    if len(cells) >= 4:
                        task_content = cells[0]
                        if task_content and task_content != "ï¼ˆè¨˜è¼‰ãªã—ï¼‰":
                            subsections.append({
                                "subtitle": f"ã‚¢ã‚¯ã‚·ãƒ§ãƒ³: {task_content}",
                                "content": f"ã‚¿ã‚¹ã‚¯: {task_content}\næ‹…å½“è€…: {cells[1]}\næœŸé™: {cells[2]}\nãƒªã‚½ãƒ¼ã‚¹: {cells[3]}",
                                "content_type": "action_item"
                            })
        
        else:
            # ãã®ä»–ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¯ãã®ã¾ã¾
            subsections.append({
                "subtitle": "",
                "content": content,
                "content_type": "text"
            })
        
        return subsections
    
    def create_chunks(self, parsed_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        è§£æã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚’ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²
        
        Args:
            parsed_data: è§£æã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿
            
        Returns:
            ãƒãƒ£ãƒ³ã‚¯ã®ãƒªã‚¹ãƒˆ
        """
        chunks = []
        chunk_id = 0
        file_name = parsed_data["file_name"]        
        # åŸºæœ¬æƒ…å ±ãƒãƒ£ãƒ³ã‚¯
        if parsed_data["metadata"]:
            chunks.append({
                "id": f"{file_name}_chunk_{chunk_id:04d}",
                "content": f"ä¼šè­°è­°äº‹éŒ²: {parsed_data['file_name']}\n\nåŸºæœ¬æƒ…å ±:\n" + 
                          "\n".join([f"{k}: {v}" for k, v in parsed_data["metadata"].items()]),
                "metadata": {
                    "type": "meeting_info",
                    "file_name": parsed_data["file_name"],
                    "chunk_type": "header",
                    **parsed_data["metadata"]
                }
            })
            chunk_id += 1
        
        # ã‚»ã‚¯ã‚·ãƒ§ãƒ³ãƒãƒ£ãƒ³ã‚¯
        for section in parsed_data["sections"]:
            chunks.append({
                "id": f"{file_name}_chunk_{chunk_id:04d}",
                "content": f"{section['title']}\n\n{section['subtitle']}\n\n{section['content']}",
                "metadata": {
                    "type": section["type"],
                    "title": section["title"],
                    "subtitle": section["subtitle"],
                    "content_type": section["content_type"],
                    "file_name": parsed_data["file_name"],
                    "chunk_type": "section"
                }
            })
            chunk_id += 1
        
        return chunks
    
    def add_to_chromadb(self, chunks: List[Dict[str, Any]]) -> None:
        """
        ãƒãƒ£ãƒ³ã‚¯ã‚’ChromaDBã«è¿½åŠ 
        
        Args:
            chunks: ãƒãƒ£ãƒ³ã‚¯ã®ãƒªã‚¹ãƒˆ
        """
        if not chunks:
            print("è¿½åŠ ã™ã‚‹ãƒãƒ£ãƒ³ã‚¯ãŒã‚ã‚Šã¾ã›ã‚“")
            return

        if not self.embedding_model:
            raise RuntimeError("åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚ç™»éŒ²ã‚’ä¸­æ­¢ã—ã¾ã™ã€‚")

        # æ—¢å­˜ã®ãƒ‡ãƒ¼ã‚¿ã‚’ç¢ºèª
        existing_count = self.collection.count()
        print(f"æ—¢å­˜ã®ãƒ‡ãƒ¼ã‚¿æ•°: {existing_count}")
        
        # æ–°ã—ã„ãƒãƒ£ãƒ³ã‚¯ã‚’è¿½åŠ 
        documents = []
        metadatas = []
        ids = []
        embeddings = []
        
        print("ğŸ”„ åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’ç”Ÿæˆä¸­...")
        
        for i, chunk in enumerate(chunks):
            documents.append(chunk["content"])
            metadatas.append(chunk["metadata"])
            ids.append(chunk["id"])
            
            # åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’ç”Ÿæˆ
            try:
                embedding = self.embedding_model.encode(chunk["content"]).tolist()
                if len(embedding) != self.embedding_dimension:
                    raise ValueError(f"ãƒãƒ£ãƒ³ã‚¯{i+1}ã®åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã®æ¬¡å…ƒæ•°ãŒä¸æ­£: {len(embedding)} (æœŸå¾…å€¤: {self.embedding_dimension})")
                embeddings.append(embedding)
                if (i + 1) % 5 == 0:
                    print(f"  ğŸ“Š {i + 1}/{len(chunks)}å€‹ã®åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’ç”Ÿæˆå®Œäº†")
            except Exception as e:
                raise RuntimeError(f"ãƒãƒ£ãƒ³ã‚¯{i+1}ã®åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ç”Ÿæˆã«å¤±æ•—: {e}")

        print(f"âœ… {len(chunks)}å€‹ã®åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã®ç”Ÿæˆå®Œäº†")
        
        # ChromaDBã«è¿½åŠ 
        try:
            print("ğŸ”„ ChromaDBã«ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ ä¸­...")
            self.collection.add(
                documents=documents,
                metadatas=metadatas,
                ids=ids,
                embeddings=embeddings
            )
            print(f"âœ… {len(chunks)}å€‹ã®ãƒãƒ£ãƒ³ã‚¯ã‚’ChromaDBã«è¿½åŠ ã—ã¾ã—ãŸ")
            print(f"ç¾åœ¨ã®ç·ãƒ‡ãƒ¼ã‚¿æ•°: {self.collection.count()}")

        except Exception as e:
            print(f"âŒ ChromaDBã¸ã®è¿½åŠ ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            import traceback
            traceback.print_exc()
            
            # ã‚¨ãƒ©ãƒ¼ã®è©³ç´°ã‚’ç¢ºèª
            print("\nğŸ” ã‚¨ãƒ©ãƒ¼ã®è©³ç´°åˆ†æ:")
            print(f"  ãƒãƒ£ãƒ³ã‚¯æ•°: {len(chunks)}")
            print(f"  åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«æ•°: {len(embeddings)}")
            print(f"  æœŸå¾…ã•ã‚Œã‚‹æ¬¡å…ƒæ•°: {self.embedding_dimension}")
            
            if embeddings:
                print(f"  æœ€åˆã®åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã®æ¬¡å…ƒæ•°: {len(embeddings[0])}")
                print(f"  æœ€å¾Œã®åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã®æ¬¡å…ƒæ•°: {len(embeddings[-1])}")
            
            raise e
    
    def process_file(self, file_path: str) -> None:
        """
        ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ã—ã¦ChromaDBã«ä¿å­˜
        
        Args:
            file_path: å‡¦ç†ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        """
        print(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ä¸­: {file_path}")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª
        if not os.path.exists(file_path):
            print(f"ã‚¨ãƒ©ãƒ¼: ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {file_path}")
            return
        
        # Markdownãƒ•ã‚¡ã‚¤ãƒ«ã‚’è§£æ
        parsed_data = self.parse_markdown_file(file_path)
        print(f"è§£æå®Œäº†: {len(parsed_data['sections'])}å€‹ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’æ¤œå‡º")
        
        # ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²
        chunks = self.create_chunks(parsed_data)
        print(f"ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²å®Œäº†: {len(chunks)}å€‹ã®ãƒãƒ£ãƒ³ã‚¯ã‚’ä½œæˆ")
        
        # ChromaDBã«ä¿å­˜
        self.add_to_chromadb(chunks)
        
        print("å‡¦ç†å®Œäº†!")
    
    def search(self, query: str, n_results: int = 5) -> List[Dict[str, Any]]:
        """
        ã‚¯ã‚¨ãƒªã§æ¤œç´¢
        
        Args:
            query: æ¤œç´¢ã‚¯ã‚¨ãƒª
            n_results: å–å¾—ã™ã‚‹çµæœæ•°
            
        Returns:
            æ¤œç´¢çµæœã®ãƒªã‚¹ãƒˆ
        """
        try:
            # åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ãŒã‚ã‚‹å ´åˆã¯ã€ã‚¯ã‚¨ãƒªã®åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’ç”Ÿæˆ
            if self.embedding_model:
                query_embedding = self.embedding_model.encode(query).tolist()
                
                # åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã®æ¬¡å…ƒæ•°ã‚’ç¢ºèª
                if len(query_embedding) != self.embedding_dimension:
                    print(f"âš ï¸  ã‚¯ã‚¨ãƒªã®åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã®æ¬¡å…ƒæ•°ãŒä¸æ­£: {len(query_embedding)} (æœŸå¾…å€¤: {self.embedding_dimension})")
                    # æ¬¡å…ƒæ•°ãŒåˆã‚ãªã„å ´åˆã¯ã‚¼ãƒ­ãƒ™ã‚¯ãƒˆãƒ«ã‚’ä½¿ç”¨
                    query_embedding = [0.0] * self.embedding_dimension
                
                # åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’ä½¿ç”¨ã—ã¦æ¤œç´¢
                results = self.collection.query(
                    query_embeddings=[query_embedding],
                    n_results=n_results,
                    include=["embeddings", "documents", "metadatas", "distances"]
                )
            else:
                # åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ãŒãªã„å ´åˆã¯ã€ãƒ†ã‚­ã‚¹ãƒˆã‚¯ã‚¨ãƒªã‚’ä½¿ç”¨
                print("âš ï¸  åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ãŒåˆ©ç”¨ã§ããªã„ãŸã‚ã€ãƒ†ã‚­ã‚¹ãƒˆã‚¯ã‚¨ãƒªã‚’ä½¿ç”¨ã—ã¾ã™")
                results = self.collection.query(
                    query_texts=[query],
                    n_results=n_results,
                    include=["embeddings", "documents", "metadatas", "distances"]
                )
            
            return results
            
        except Exception as e:
            print(f"âŒ æ¤œç´¢ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            import traceback
            traceback.print_exc()
            
            # ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯ç©ºã®çµæœã‚’è¿”ã™
            return {
                "documents": [[]],
                "metadatas": [[]],
                "distances": [[]],
                "embeddings": [[]]
            }
    
    def get_collection_info(self) -> Dict[str, Any]:
        """
        ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã®æƒ…å ±ã‚’å–å¾—
        
        Returns:
            ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³æƒ…å ±
        """
        count = self.collection.count()
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®çµ±è¨ˆã‚’å–å¾—
        all_data = self.collection.get()
        type_counts = {}
        file_counts = {}
        
        for metadata in all_data["metadatas"]:
            if metadata:
                # ã‚¿ã‚¤ãƒ—åˆ¥ã‚«ã‚¦ãƒ³ãƒˆ
                chunk_type = metadata.get("chunk_type", "unknown")
                type_counts[chunk_type] = type_counts.get(chunk_type, 0) + 1
                
                # ãƒ•ã‚¡ã‚¤ãƒ«åˆ¥ã‚«ã‚¦ãƒ³ãƒˆ
                file_name = metadata.get("file_name", "unknown")
                file_counts[file_name] = file_counts.get(file_name, 0) + 1
        
        return {
            "total_count": count,
            "type_counts": type_counts,
            "file_counts": file_counts
        }


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="è­°äº‹éŒ²ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ChromaDBã«ä¿å­˜ã™ã‚‹ãƒ—ãƒ­ã‚°ãƒ©ãƒ ")
    parser.add_argument("file_path", nargs='?', help="å‡¦ç†ã™ã‚‹Markdownãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹")
    parser.add_argument("--db-path", default="./chroma_db", help="ChromaDBã®ä¿å­˜ãƒ‘ã‚¹")
    parser.add_argument("--search", help="æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’å®Ÿè¡Œ")
    parser.add_argument("--info", action="store_true", help="ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³æƒ…å ±ã‚’è¡¨ç¤º")
    
    args = parser.parse_args()
    
    # ãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼ã‚’åˆæœŸåŒ–
    processor = MeetingRAGProcessor(args.db_path)
    
    if args.info:
        # ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³æƒ…å ±ã‚’è¡¨ç¤º
        info = processor.get_collection_info()
        print("=== ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³æƒ…å ± ===")
        print(f"ç·ãƒ‡ãƒ¼ã‚¿æ•°: {info['total_count']}")
        print("\nã‚¿ã‚¤ãƒ—åˆ¥ã‚«ã‚¦ãƒ³ãƒˆ:")
        for chunk_type, count in info['type_counts'].items():
            print(f"  {chunk_type}: {count}")
        print("\nãƒ•ã‚¡ã‚¤ãƒ«åˆ¥ã‚«ã‚¦ãƒ³ãƒˆ:")
        for file_name, count in info['file_counts'].items():
            print(f"  {file_name}: {count}")
        return
    
    if args.search:
        # æ¤œç´¢ã‚’å®Ÿè¡Œ
        print(f"æ¤œç´¢ã‚¯ã‚¨ãƒª: {args.search}")
        results = processor.search(args.search)
        
        print("\n=== æ¤œç´¢çµæœ ===")
        for i, (doc, metadata, distance) in enumerate(zip(
            results["documents"][0], 
            results["metadatas"][0], 
            results["distances"][0]
        )):
            print(f"\n--- çµæœ {i+1} (è·é›¢: {distance:.4f}) ---")
            print(f"ã‚¿ã‚¤ãƒ—: {metadata.get('type', 'unknown')}")
            print(f"ã‚¿ã‚¤ãƒˆãƒ«: {metadata.get('title', 'N/A')}")
            if metadata.get('subtitle'):
                print(f"ã‚µãƒ–ã‚¿ã‚¤ãƒˆãƒ«: {metadata['subtitle']}")
            print(f"ãƒ•ã‚¡ã‚¤ãƒ«: {metadata.get('file_name', 'N/A')}")
            print(f"å†…å®¹: {doc[:200]}...")
        return
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ãŒæŒ‡å®šã•ã‚Œã¦ã„ãªã„å ´åˆ
    if not args.file_path:
        print("ä½¿ç”¨æ–¹æ³•:")
        print("  ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†: python meeting_rag_processor.py <ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹>")
        print("  æ¤œç´¢å®Ÿè¡Œ: python meeting_rag_processor.py --search <æ¤œç´¢ã‚¯ã‚¨ãƒª>")
        print("  æƒ…å ±è¡¨ç¤º: python meeting_rag_processor.py --info")
        print("  ãƒ˜ãƒ«ãƒ—: python meeting_rag_processor.py --help")
        return
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†
    processor.process_file(args.file_path)


if __name__ == "__main__":
    main()
